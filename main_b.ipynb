{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tugas Besar IF3270 Pembelajaran Mesin Bag.B\n",
    "\n",
    "Anggota Kelompok:\n",
    "1. 13520039 - Rozan Fadhil Al Hafidz\n",
    "1. 13520054 - Farrel Farandieka Fibriyanto\n",
    "1. 13520103 - Amar Fadil\n",
    "1. 13520105 - Malik Akbar Hashemi Rafsanjani"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teori Dasar\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "![Gradient Descent](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*P7z2BKhd0R-9uyn9ThDasA.png)\n",
    "\n",
    "Sumber gambar : [Kapil, D.](https://medium.com/@divakar_239/stochastic-vs-batch-gradient-descent-8820568eada1)\n",
    "\n",
    "Gradient descent adalah aloritma optimisasi yang sering digunakan untuk mencari bobot atau koefisien dalam algoritma pembelajaran mesin, seperti pada artificial neural networks dan logistic regression.\n",
    "\n",
    "Tujuan dari algoritma ini adalah untuk menemukan parameter model, seperti koefisien atau bobot, yang menghasilkan error minimum pada dataset train. Algoritma ini melakukan perubahan terhadap model dengan menggesernya ke arah yang berlawanan dengan gradien dari fungsi error. Hal ini dilakukan secara berulang-ulang sampai model yang dihasilkan sudah cukup baik. Oleh karena itu, algoritma ini disebut \"gradient descent\".\n",
    "\n",
    "Berikut ini adalah variasi dari gradient descent:\n",
    "\n",
    "#### Stochastic Gradient Descent\n",
    "Stochastic gradient descent, atau biasa disingkat SGD, adalah variasi dari gradient algoritma gradient descent yang mengkalkulasi error dan mengupdate model untuk setiap data pada dataset training.\n",
    "\n",
    "#### Batch Gradient Descent\n",
    "Batch gradient descent adalah variasi dari algoritma gradient descent yang mengkalkulasi error untuk setiap data pada dataset training, tetapi hanya mengupdate model setelah semua data pada dataset training sudah dievaluasi.\n",
    "\n",
    "Satu siklus mengiterasi seluruh dataset training (disebut sebagai training epoch). Oleh karena itu, batch gradient descent melakukan update model pada akhir setiap training epoch.\n",
    "\n",
    "#### Mini-Batch Gradient Descent\n",
    "Mini-batch gradient descent adalah variasi algoritma gradient descent yang membagi dataset training menjadi beberapa batch kecil yang digunakan untuk mengkalkulasi error model dan mengupdate koefisien model.\n",
    "\n",
    "Mini-batch gradient descent mencari keseimbangan antara robustness dari stochastic gradient descent dan efisiensi dari batch gradient descent. Ini adalah implementasi gradient descent yang paling umum digunakan dalam bidang deep learning.\n",
    "\n",
    "Sumber: [Brownlee, J.](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/)\n",
    "\n",
    "\n",
    "### Perbandingan SGD, BGD, dan MBGD\n",
    "![Perbandingan](https://miro.medium.com/v2/resize:fit:720/format:webp/1*_ctmL9Ya0DpppDFbiYa7VQ.png)\n",
    "\n",
    "Sumber gambar : [Sweta](https://sweta-nit.medium.com/batch-mini-batch-and-stochastic-gradient-descent-e9bc4cacd461)\n",
    "\n",
    "Stochastic gradient descend adalah Mini-batch gradient descend dengan ukuran batch = 1, sedangkan Batch gradient descend adalah Mini-batch gradient descend dengan ukuran batch = jumlah data. Mini-batch gradient descend lebih sering dipilih karena lebih cepat dan lebih stabil dibandingkan dengan stochastic gradient descend karena tidak terlalu sering mengupdate bobot.\n",
    "\n",
    "Sumber : [Sweta](https://sweta-nit.medium.com/batch-mini-batch-and-stochastic-gradient-descent-e9bc4cacd461)\n",
    "\n",
    "\n",
    "### Batch Size\n",
    "Batch size (ukuran batch) mendefinisikan jumlah sampel yang akan dipropagasi melalui network. Contohnya, jika terdapat 1050 data training dan anda akan menggunakan batch_size sebesar 100, maka algoritma akan mengambil 100 sampel pertama (dari sampel ke-1 sampai ke-100) dari data training dan melatih network. Selanjutnya, algoritma akan mengambil 100 sampel kedua (dari sampel ke-101 sampai ke-200) dan melatih network lagi. Hal ini akan terus dilakukan sampai semua sampel telah dipropagasi melalui network.\n",
    "\n",
    "Keuntungan menggunakan batch size yang lebih kecil dari jumlah semua sampel adalah program memerlukan lebih sedikit memori. Selain itu, biasanya proses pelatihan network akan lebih cepat dengan menggunakan mini-batch. Hal ini dikarenakan bobot akan diupdate setelah setiap propagasi.\n",
    "\n",
    "Sumber : [Sweta](https://sweta-nit.medium.com/batch-mini-batch-and-stochastic-gradient-descent-e9bc4cacd461)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate Program"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kelas activations akan menyimpan fungsi aktivasi yang dapat dipakai oleh kelas neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kelas JsonReader bertanggung jawab untuk membaca file json. Dengan kelas ini, program dapat me-load model-model yang telah ditetapkan di file json eksternal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class JsonReader:\n",
    "    def __init__(self, filename: str):\n",
    "        self.filename = filename\n",
    "        self.data = None\n",
    "\n",
    "    def read(self):\n",
    "        with open(self.filename, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        return self.data\n",
    "\n",
    "    def get(self, index: int| str):\n",
    "        return self.data[index]\n",
    "\n",
    "    def length(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kelas JsonParser bertanggung jawab untuk mem-parse file json dengan format yang ditentukan oleh asisten. Dengan kelas ini, program dapat mem-parse model-model yang telah ditetapkan di file json eksternal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json_reader import JsonReader\n",
    "from model_types import *\n",
    "\n",
    "class JsonParser:\n",
    "    def parse_model_config(self, json_path: str) -> ModelConfig:\n",
    "        model_config: ModelConfig = JsonReader(json_path).read()\n",
    "        return model_config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ModelConfig adalah kelas model yang bertugas menyimpan konfigurasi model yang akan digunakan dengan struktur data yang telah ditentukan oleh asisten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "class Layers(TypedDict):\n",
    "    number_of_neurons: int\n",
    "    activation_function : str\n",
    "\n",
    "class ModelData(TypedDict):\n",
    "    input_size: int\n",
    "    layers: Layers\n",
    "\n",
    "class LearningParameters(TypedDict): \n",
    "    learning_rate: float\n",
    "    batch_size: int\n",
    "    max_iteration: int\n",
    "    error_threshold: float\n",
    "\n",
    "class Case(TypedDict):\n",
    "    model: ModelData\n",
    "    input: list[list[list[float]]]\n",
    "    initial_weights: list[list[list[float]]]\n",
    "    target: list[list[list[float]]]\n",
    "    learning_parameters: LearningParameters\n",
    "\n",
    "class Expect(TypedDict):\n",
    "    stopped_by: str\n",
    "    final_weights: list[list[list[float]]]\n",
    "\n",
    "\n",
    "class ModelConfig(TypedDict):\n",
    "    case: Case\n",
    "    expect: Expect\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ModelFactory adalah kelas yang bertugas untuk membuat model berdasarkan konfigurasi dari ModelConfig. Selain itu, kelas ini juga bisa me-load model dari file eksternal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json_reader import JsonReader\n",
    "from ann import Model, Layer\n",
    "from model_types import ModelConfig\n",
    "import pickle\n",
    "\n",
    "class ModelFactory:\n",
    "    def build(self, model_config: ModelConfig) -> Model:\n",
    "        case = model_config[\"case\"]\n",
    "        learning_parameters = case[\"learning_parameters\"]\n",
    "        model = Model(\n",
    "            learning_rate=learning_parameters[\"learning_rate\"],\n",
    "            batch_size=learning_parameters[\"batch_size\"],\n",
    "            max_iterations=learning_parameters[\"max_iteration\"],\n",
    "            error_threshold=learning_parameters[\"error_threshold\"]\n",
    "        )\n",
    "        for i in range(len(case[\"model\"][\"layers\"])):\n",
    "            layer = case[\"model\"][\"layers\"][i]\n",
    "            weights = case[\"initial_weights\"][i][1:]\n",
    "            weights = [list(x) for x in zip(*weights)]\n",
    "            for j in range(len(weights)):\n",
    "                weights[j].append(case[\"initial_weights\"][i][0][j])\n",
    "            model.add(Layer(\n",
    "                layer[\"number_of_neurons\"],\n",
    "                input_shape=case[\"model\"].get('input_size'),\n",
    "                activation=layer['activation_function'],\n",
    "                weights=weights,\n",
    "                bias=1,\n",
    "            ))\n",
    "        return model\n",
    "    \n",
    "    def load(self, path: str) -> Model:\n",
    "        with open(path, 'rb') as f:\n",
    "            return pickle.load(f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
